## 马尔可夫过程

**马尔可夫性质**：指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。
$$
p(X_{{t+1}}​=x_{t+1}​∣X_{0: t}​=x_{0: t})=p(X_{t+1}=x_{{t+1}}​∣X_{t}​=x_{t}​)
$$
### 马尔可夫链
马尔可夫过程是一组具有马尔可夫性质的随机变量序列 $s_{1},\dots s_{t}$.其中下一个时刻的状态$s_{t+1}$​只取决于当前状态 $s_{t}$。记$h_{t} = \{s_{1} \dots s_{t}\}$, 那么
$$
p(s_{t+1}|s_{t}) = p(s_{t+1}|h_{t})
$$
离散时间的马尔可夫过程被称为马尔科夫链，下面是一个马尔科夫链的例子

![[Pasted image 20250819002811.png]]​
我们可以用**状态转移矩阵（state transition matrix）**$P$来描述状态转移
$$
P = \begin{bmatrix}
p(s_{1}|s_{1})&p(s_{1}|s_{2})& \dots &p(s_{1}|s_{N}) \\
\vdots & \vdots & \ddots & \vdots \\
p(s_{N}|s_{1}) & p(s_{N}|s_{2}) & \dots & p(s_{N}|s_{N})
\end{bmatrix}
$$

## 马尔可夫奖励过程
马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward function）。奖励函数 $R$ 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子 $\gamma$ 。如果状态数是有限的，那么 $R$ 可以是一个向量。
**范围(horizon)**: 是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的.
**回报**：可以定义为奖励的逐步叠加。时刻$t$以后的奖励序列是$r_{t+1},r_{{t+2}},\dots$,则回报为$G_{t} = r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots+\gamma^{T-t-1}r_{T}$
**状态价值函数**：定义成回报的期望，即
$$
V^t(s) = \mathbb{E}[G_{t}|s_{t} = s]
$$
使用折扣因子的原因如下。第一，有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。第二，我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。第三，如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。最后，我们也更想得到即时奖励。有些时候可以把折扣因子设为 0，我们就只关注当前的奖励。我们也可以把折扣因子设为 1，对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数（hyperparameter）来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。
如何计算状态价值函数？可以采用蒙特卡洛采样法

### 贝尔曼方程

$$
V(s) = R(s) +\gamma\sum_{s'\in S}p(s'|s)V(s')
$$

推导过程如下：![image-20250822204816149](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/image-20250822204816149.png)

### 计算马尔可夫奖励过程的迭代算法

对于状态数比较小的马尔科夫奖励过程，我们可以直接解出解析解
$$
V = R+\gamma PV\\
V = (1-\gamma P)^{-1}R
$$
一般来说矩阵求逆过程时间复杂度是$O(N^3)$,因此对于状态数较多的马尔科夫奖励过程，一般使用迭代的方法。比如：动态规划的方法，蒙特卡洛的方法，时序差分学习（temporal-difference learning，TD learning）的方法

**蒙特卡洛算法**：蒙特卡洛方法就是当得到一个马尔可夫奖励过程后，我们可以从某个状态开始，把小船放到状态转移矩阵里面，让它“随波逐流”，这样就会产生一个轨迹。产生一个轨迹之后，就会得到一个奖励，那么直接把折扣的奖励即回报 $g$ 算出来。算出来之后将它积累起来，得到回报$G_t$。 当积累了一定数量的轨迹之后，我们直接用 $G_t$ 除以轨迹数量，就会得到某个状态的价值.

![img](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/2.6.png)

**动态规划方法**：通过**自举（bootstrapping）**的方法不停地迭代贝尔曼方程，当最后更新的状态与我们上一个状态的区别并不大的时候，更新就可以停止，我们就可以输出最新的 $V'(s)$ 作为它当前的状态的价值

![img](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/2.7.png)

### 马尔可夫决策过程

相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。状态转移的条件变为了$P(s_{t+1} = s' | s_{t} = s,a_{t}=a)$。对于奖励函数，也变为了$R(s_t=s,a_t=a) = \mathbb{E}[r_t|s_t=s,a_t=a]$

#### 马尔可夫决策过程中的策略

策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即$\pi(a|s) = p(a_t=a|s_t=s)$。假设概率函数是平稳的（stationary），不同时间点，我们采取的动作其实都是在对策略函数进行采样。

已知马尔可夫决策过程和策略 π*π*，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。$P_{\pi}(s'|s) = \sum_{s' \in S}\pi(a|s)p(s'|s,a)$.对于奖励函数，我们也可以把a去掉，得到类似于马尔可夫奖励过程中的奖励函数，$r_{\pi}(s) = \sum_{a \in A}\pi(a|s)R(s,a)$

#### 决策过程中的价值函数

马尔可夫决策过程中的价值函数可定义为
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t|s_t=s]
$$
其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数

**Q函数：动作价值函数**：$Q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|s_t=s,a_t=a]$。Q函数的贝尔曼方程推导如下：

![image-20250822211955005](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/image-20250822211955005.png)

对Q函数的动作进行加和，我们就能得到Q函数与状态价值函数V的关系
$$
V_{\pi}(s) = \sum_{a \in A}\pi(a|s)Q(s,a)
$$

#### 贝尔曼期望方程

我们可以把状态价值函数和 Q 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。通过对状态价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程————**贝尔曼期望方程（Bellman expectation equation）**
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[r_{t+1}+\gamma V_{\pi}(s_{t+1})|s_t=s]\\
V_{\pi}(s) = \sum_{a\in A}\pi(a|s)(R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{\pi}(s'))
$$
上式代表当前状态的价值与未来状态价值之间的关联
$$
Q_{\pi}(s,a) = \mathbb{E}_{\pi}[r_{t+1}+\gamma Q_{\pi}(s_{t+1},a_{t+1})|s_t=s,a_t=a]\\
Q_{\pi}(s,a) = R(s,a)+\gamma \sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi(a'|s')Q_{\pi}(s',a')
$$
上式代表当前时刻的 Q 函数与未来时刻的 Q 函数之间的关联



**策略评估**：已知马尔可夫决策过程以及要采取的策略$\pi$ ，计算价值函数 $V_{\pi}(s)$ 的过程

**预测**：输入是一个马尔可夫决策过程，$<S,A,P,R,\gamma>$和策略$\pi$, 输出是价值函数$V_{\pi}$。指给定一个马尔可夫决策过程以及一个策略 $\pi$ ，计算它的价值函数，也就是计算每个状态的价值

**控制**：输入是马尔可夫决策过程 $<S,A,P,R,\gamma>$，输出是最佳价值函数（optimal value function）$V^*$ 和最佳策略（optimal policy）$\pi^*$。控制就是我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数以及最佳策略

马尔可夫的决策过程的策略评估可以直接使用$V_{\pi}(s) = \sum_{a\in A}\pi(a|s)(R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{\pi}(s'))$这个式子进行不断的迭代，直至最后收敛即可

#### 马尔可夫决策过程控制

最佳价值函数是指，我们搜索一种策略$\pi$ 让每个状态的价值最大。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以认为某个马尔可夫决策过程的环境可解。在这种情况下，最佳价值函数是一致的，环境中可达到的上限的值是一致的，但这里可能有多个最佳策略，多个最佳策略可以取得相同的最佳价值。

如果我们找到了最佳价值函数，那么我们可以通过max Q函数来获得最佳策略。
$$
\pi^*(a|s) = \begin{cases}
1 & a=argmax_{a\in A}Q^*(s,a)\\
0 & others
\end{cases}
$$
当Q函数收敛后，因为 Q 函数是关于状态与动作的函数，所以如果在某个状态采取某个动作，可以使得 Q 函数最大化，那么这个动作就是最佳的动作。如果我们能优化出一个 Q 函数 Q∗(s,a)*Q*∗(*s*,*a*)，就可以直接在 Q 函数中取一个让 Q 函数值最大化的动作的值，就可以提取出最佳策略。

**怎么解决控制问题：策略迭代和价值迭代**

#### 策略迭代

策略迭代由两个部分组成，策略评估和策略改进。第一个步骤是策略评估，当前我们在优化策略$\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。 第二个步骤是策略改进，得到 状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q 函数后，我们直接对 Q 函数进行最大化，通过在 Q 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。![img](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/2.21.png)

如何进行策略改进？

得到状态价值函数后，我们就可以通过奖励函数以及状态转移函数来计算 Q 函数：
$$
Q_{\pi_i}(s,a) = R(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V_{\pi_i}(s')
$$
对于每个状态，策略改进会得到它的新一轮的策略，对于每个状态，我们取使它得到最大值的动作。即：$\pi_{i+1}(s) = argmax_{a}Q_{\pi_i}(s,a)$

**贝尔曼最优方程**

当我们一直采取 arg max 操作的时候，我们会得到一个单调的递增。通过采取这种贪心操作（arg max 操作），我们就会得到更好的或者不变的策略，而不会使价值函数变差。所以当改进停止后，我们就会得到一个最佳策略。当改进停止后，我们取让 Q 函数值最大化的动作，Q 函数就会直接变成价值函数，即$V_{\pi}(s) = max_aQ^*(s,a)$

**价值迭代**

![image-20250822225826007](%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B.assets/image-20250822225826007.png)

